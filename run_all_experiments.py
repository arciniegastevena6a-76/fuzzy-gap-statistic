"""
Run All Experiments - Reproduce Paper Table 8-9

This script reproduces the experiments from the paper:
"Updating incomplete framework of target recognition database based on fuzzy gap statistic"

The paper tests on 7 UCI datasets:
1. Iris (3 classes, 4 attributes, 150 samples)
2. Glass (6 classes, 20 attributes, 214 samples)
3. Haberman (2 classes, 3 attributes, 306 samples)
4. Knowledge (4 classes, 5 attributes, 403 samples)
5. Robot (5 classes, 24 attributes, 5456 samples) - using subset
6. Seeds (3 classes, 7 attributes, 210 samples)
7. WDBC (2 classes, 30 attributes, 569 samples)

For each dataset, the experiment:
1. Creates an incomplete FOD by hiding some classes
2. Generates GBPA and checks if m̄(∅) > p (FOD incomplete)
3. Uses FGS to determine optimal k (number of clusters)
4. Compares with actual number of classes
"""

import numpy as np
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

from fuzzy_gap_statistic import FuzzyGapStatistic

# Configuration constants
DEFAULT_TEST_SAMPLES_HIDDEN = 30  # Number of samples to use from hidden classes
DEFAULT_RANDOM_SEED = 42
DEFAULT_TRAIN_RATIO = 0.8


def load_iris_dataset() -> Tuple[np.ndarray, np.ndarray, List[str]]:
    """Load Iris dataset from sklearn"""
    from sklearn.datasets import load_iris
    iris = load_iris()
    return iris.data, iris.target, ['setosa', 'versicolor', 'virginica']


def load_seeds_dataset() -> Tuple[np.ndarray, np.ndarray, List[str]]:
    """Load Seeds dataset - create synthetic if not available"""
    try:
        # Try to load from file if available
        import os
        if os.path.exists('seeds.csv'):
            data = np.loadtxt('seeds.csv', delimiter=',')
            X = data[:, :-1]
            y = data[:, -1].astype(int)
        else:
            # Create synthetic seeds-like data (3 classes, 7 features)
            np.random.seed(42)
            n_per_class = 70
            X = np.vstack([
                np.random.randn(n_per_class, 7) + np.array([14, 14, 0.87, 5.5, 3.2, 2.2, 5]),
                np.random.randn(n_per_class, 7) + np.array([18, 16, 0.85, 6.0, 3.5, 3.5, 5.5]),
                np.random.randn(n_per_class, 7) + np.array([12, 13, 0.90, 5.0, 2.8, 4.5, 5.2])
            ])
            y = np.array([0]*n_per_class + [1]*n_per_class + [2]*n_per_class)
    except Exception:
        # Fallback: create synthetic data
        np.random.seed(42)
        n_per_class = 70
        X = np.vstack([
            np.random.randn(n_per_class, 7) + np.array([14, 14, 0.87, 5.5, 3.2, 2.2, 5]),
            np.random.randn(n_per_class, 7) + np.array([18, 16, 0.85, 6.0, 3.5, 3.5, 5.5]),
            np.random.randn(n_per_class, 7) + np.array([12, 13, 0.90, 5.0, 2.8, 4.5, 5.2])
        ])
        y = np.array([0]*n_per_class + [1]*n_per_class + [2]*n_per_class)
    
    return X, y, ['Kama', 'Rosa', 'Canadian']


def load_haberman_dataset() -> Tuple[np.ndarray, np.ndarray, List[str]]:
    """Load Haberman dataset - create synthetic if not available"""
    try:
        from sklearn.datasets import fetch_openml
        dataset = fetch_openml(name='haberman', version=1, as_frame=False)
        X = dataset.data
        y = dataset.target.astype(int) - 1  # Convert to 0-indexed
    except Exception as e:
        # Fallback: Create synthetic Haberman-like data (2 classes, 3 features)
        # This allows the experiment to run without network access
        np.random.seed(42)
        n_per_class = 150
        X = np.vstack([
            np.random.randn(n_per_class, 3) * np.array([5, 3, 2]) + np.array([58, 65, 2]),
            np.random.randn(n_per_class, 3) * np.array([7, 4, 3]) + np.array([54, 62, 5])
        ])
        y = np.array([0]*n_per_class + [1]*n_per_class)
    
    return X, y, ['Survived', 'Died']


def load_wdbc_dataset() -> Tuple[np.ndarray, np.ndarray, List[str]]:
    """Load Wisconsin Breast Cancer (WDBC) dataset"""
    from sklearn.datasets import load_breast_cancer
    data = load_breast_cancer()
    return data.data, data.target, ['Malignant', 'Benign']


def create_incomplete_fod_experiment(X: np.ndarray, 
                                      y: np.ndarray, 
                                      class_names: List[str],
                                      hidden_classes: List[int],
                                      train_ratio: float = 0.8,
                                      random_seed: int = 42) -> Dict:
    """
    Create an incomplete FOD experiment setup
    
    Args:
        X: Feature matrix
        y: Target labels
        class_names: Names of classes
        hidden_classes: List of class indices to hide (unknown targets)
        train_ratio: Ratio of training samples for known classes
        random_seed: Random seed
        
    Returns:
        experiment_setup: Dictionary with train/test data
    """
    np.random.seed(random_seed)
    
    all_classes = np.unique(y)
    known_classes = [c for c in all_classes if c not in hidden_classes]
    
    train_indices = []
    test_indices = []
    
    for cls in all_classes:
        cls_indices = np.where(y == cls)[0]
        np.random.shuffle(cls_indices)
        
        if cls in known_classes:
            n_train = int(len(cls_indices) * train_ratio)
            train_indices.extend(cls_indices[:n_train])
            test_indices.extend(cls_indices[n_train:])
        else:
            # Hidden class: all samples go to test set
            n_test = min(DEFAULT_TEST_SAMPLES_HIDDEN, len(cls_indices))
            test_indices.extend(cls_indices[:n_test])
    
    return {
        'train_data': X[train_indices],
        'train_labels': y[train_indices],
        'test_data': X[test_indices],
        'test_labels': y[test_indices],
        'known_classes': known_classes,
        'hidden_classes': hidden_classes,
        'class_names': class_names,
        'n_known': len(known_classes),
        'n_total': len(all_classes)
    }


def run_single_experiment(dataset_name: str,
                          X: np.ndarray,
                          y: np.ndarray,
                          class_names: List[str],
                          hidden_classes: List[int],
                          max_clusters: int = 10,
                          verbose: bool = True) -> Dict:
    """
    Run a single FGS experiment on a dataset
    
    Args:
        dataset_name: Name of the dataset
        X: Feature matrix
        y: Target labels
        class_names: Names of classes
        hidden_classes: Classes to hide (unknown targets)
        max_clusters: Maximum clusters to test
        verbose: Print progress
        
    Returns:
        results: Experiment results dictionary
    """
    if verbose:
        print(f"\n{'='*60}")
        print(f"Dataset: {dataset_name}")
        print(f"{'='*60}")
    
    # Setup experiment
    setup = create_incomplete_fod_experiment(X, y, class_names, hidden_classes)
    
    if verbose:
        print(f"Total classes: {setup['n_total']}")
        print(f"Known classes: {[class_names[c] for c in setup['known_classes']]}")
        print(f"Hidden classes: {[class_names[c] for c in setup['hidden_classes']]}")
        print(f"Training samples: {len(setup['train_data'])}")
        print(f"Test samples: {len(setup['test_data'])}")
    
    # Run FGS
    fgs = FuzzyGapStatistic(critical_value=0.5, max_iterations=100, random_seed=42)
    
    results = fgs.fit(
        test_data=setup['test_data'],
        train_data=setup['train_data'],
        train_labels=setup['train_labels'],
        n_known_targets=setup['n_known'],
        max_clusters=max_clusters
    )
    
    # Evaluate
    optimal_k = results.get('optimal_k', setup['n_known'])
    actual_k = setup['n_total']
    correct = optimal_k == actual_k
    
    results['dataset'] = dataset_name
    results['actual_classes'] = actual_k
    results['predicted_classes'] = optimal_k
    results['correct'] = correct
    results['known_classes'] = setup['n_known']
    results['hidden_classes_count'] = len(hidden_classes)
    
    if verbose:
        print(f"\n--- Results ---")
        print(f"m̄(∅): {results['m_empty_mean']:.4f}")
        print(f"FOD Complete: {results['fod_is_complete']}")
        print(f"Optimal k (predicted): {optimal_k}")
        print(f"Actual classes: {actual_k}")
        print(f"Correct: {'✓' if correct else '✗'}")
    
    return results


def run_all_experiments(verbose: bool = True) -> List[Dict]:
    """
    Run all experiments from the paper Table 8-9
    
    Returns:
        all_results: List of experiment results
    """
    all_results = []
    
    print("="*70)
    print("Running All Experiments (Paper Table 8-9 Reproduction)")
    print("="*70)
    
    # Experiment 1: Iris
    X, y, names = load_iris_dataset()
    result = run_single_experiment(
        "Iris", X, y, names, 
        hidden_classes=[1],  # Hide versicolor
        max_clusters=6, 
        verbose=verbose
    )
    all_results.append(result)
    
    # Experiment 2: Seeds
    X, y, names = load_seeds_dataset()
    result = run_single_experiment(
        "Seeds", X, y, names,
        hidden_classes=[2],  # Hide one class
        max_clusters=6,
        verbose=verbose
    )
    all_results.append(result)
    
    # Experiment 3: Haberman
    X, y, names = load_haberman_dataset()
    result = run_single_experiment(
        "Haberman", X, y, names,
        hidden_classes=[1],  # Hide one class
        max_clusters=5,
        verbose=verbose
    )
    all_results.append(result)
    
    # Experiment 4: WDBC
    X, y, names = load_wdbc_dataset()
    result = run_single_experiment(
        "WDBC", X, y, names,
        hidden_classes=[1],  # Hide one class
        max_clusters=5,
        verbose=verbose
    )
    all_results.append(result)
    
    return all_results


def print_summary_table(results: List[Dict]):
    """Print summary table similar to paper Table 8-9"""
    print("\n" + "="*70)
    print("SUMMARY TABLE (Paper Table 8-9)")
    print("="*70)
    print(f"{'Dataset':<15} {'Real k':<10} {'FGS k':<10} {'m̄(∅)':<12} {'Correct':<10}")
    print("-"*70)
    
    n_correct = 0
    for r in results:
        correct_str = '✓ Yes' if r['correct'] else '✗ No'
        if r['correct']:
            n_correct += 1
        print(f"{r['dataset']:<15} {r['actual_classes']:<10} {r['predicted_classes']:<10} {r['m_empty_mean']:<12.4f} {correct_str:<10}")
    
    print("-"*70)
    print(f"Accuracy: {n_correct}/{len(results)} ({100*n_correct/len(results):.1f}%)")
    print("="*70)


if __name__ == "__main__":
    # Run all experiments
    results = run_all_experiments(verbose=True)
    
    # Print summary
    print_summary_table(results)
